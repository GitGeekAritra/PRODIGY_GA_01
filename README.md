# PRODIGY_GA_01
This project showcases fine-tuning of OpenAIâ€™s GPT-2 model on a custom text dataset to generate coherent, context-aware text based on user prompts. It demonstrates how transfer learning can be used to adapt a large language model for domain-specific text generation.

Task Overview
- Fine-tune GPT-2 to generate coherent and contextually relevant text using a custom dataset.

Model Used
- GPT-2 from HuggingFace Transformers

Dataset
- Custom dataset located in `dataset/your_dataset.txt`
- Format: Plain text

Features
- Fine-tuning of GPT-2
- Text generation using trained model
- Prompt-based output

How to Run

Install Dependencies
```bash
pip install -r requirements.txt

